import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

def compute_imbalance_ratio(labels, num_classes):
    """
    ê° í´ë¼ì´ì–¸íŠ¸ì˜ ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³ , Long-Tail ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    - labels: í•´ë‹¹ í´ë¼ì´ì–¸íŠ¸ì˜ ë°ì´í„°ì…‹ (numpy array)
    - num_classes: ì´ í´ë˜ìŠ¤ ê°œìˆ˜
    
    return: (imbalance_ratio, is_long_tail)
    """
    class_counts = np.array([np.count_nonzero(labels == j) for j in range(num_classes)])
    
    # ìµœì†Œ-ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° (Imbalance Ratio)
    max_count = np.max(class_counts)
    min_count = np.min(class_counts[class_counts > 0])  # 0ì„ ì œì™¸í•œ ìµœì†Œê°’
    imbalance_ratio = max_count / min_count
    
    # ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ Long-Tail ì—¬ë¶€ íŒë‹¨ (ê¸°ë³¸ê°’: IR > 5ì´ë©´ long-tail)
    IR_threshold = 5
    is_long_tail = imbalance_ratio > IR_threshold

    return imbalance_ratio, is_long_tail, class_counts

def gini_coefficient(class_counts):
    """
    Gini ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶ˆê· í˜• ì¸¡ì •
    - class_counts: ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ (list ë˜ëŠ” numpy array)
    """
    class_counts = np.array(class_counts)
    sorted_counts = np.sort(class_counts)  # ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ì •ë ¬
    n = len(class_counts)
    cumulative = np.cumsum(sorted_counts)  # ëˆ„ì  í•©
    gini = (2 * np.sum((np.arange(n) + 1) * sorted_counts)) / (n * np.sum(sorted_counts)) - (n + 1) / n
    return gini



class DynamicLoss(nn.Module):
    def __init__(self, class_counts, gini_thresholds=(0.5, 0.7), gamma=1.0):
        """
        - class_counts: ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜
        - ir_thresholds: (ì•½í•œ long-tail ê¸°ì¤€, ê°•í•œ long-tail ê¸°ì¤€)
        - gamma: Focal Loss ê°ë§ˆ ê°’
        """
        super(DynamicLoss, self).__init__()
        self.gini = gini_coefficient(class_counts)
        self.class_counts = class_counts
        self.gamma = gamma

        if self.gini <= gini_thresholds[0]:
            self.loss_fn = FocalLoss()
            self.loss_type = "FocalLoss"
        elif self.gini <= gini_thresholds[1]:
            self.loss_fn = FocalLoss()
            self.loss_type = "FocalLoss"
        else:
            self.loss_fn = FocalLoss()  # Severe Long-Tailì—ëŠ” LDAM ì ìš©
            self.loss_type = "FocalLoss"

        print(f"Dynamic Loss - Gini: {self.gini:.3f}, Loss Type: {self.loss_fn.__class__.__name__}")

    def forward(self, outputs, targets):
        return self.loss_fn(outputs, targets)
    
    
    

class ClassBalancedLoss(nn.Module):
    def __init__(self, class_counts, reduction='mean'):
        """
        - class_counts: ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ (numpy array ë˜ëŠ” list)
        - reduction: 'mean' ë˜ëŠ” 'sum'
        """
        super(ClassBalancedLoss, self).__init__()
        self.class_weights = torch.tensor(1.0 / (class_counts + 1e-5), dtype=torch.float32)
        self.reduction = reduction

    def forward(self, outputs, targets):
        loss = F.cross_entropy(outputs, targets, weight=self.class_weights.to(outputs.device), reduction=self.reduction)
        return loss


class FocalLoss(nn.Module):
    """
    Focal Loss for multi-class classification tasks with class imbalance.
    """
    def __init__(self, alpha=None, gamma=1.0, reduction='mean'):
        """
        :param alpha: Class-wise balancing factor (can be a list, tensor, or scalar).
        :param gamma: Focusing parameter to down-weight easy examples.
        :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.
        """
        super(FocalLoss, self).__init__()
        
        if alpha is not None:
            if isinstance(alpha, (list, tuple)):
                self.alpha = torch.tensor(alpha, dtype=torch.float32)
            elif isinstance(alpha, torch.Tensor):
                self.alpha = alpha.float()
            else:
                self.alpha = torch.tensor([alpha], dtype=torch.float32)
        else:
            self.alpha = None
        
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        Compute the focal loss.
        :param inputs: Predicted logits (not softmaxed) of shape (batch_size, num_classes).
        :param targets: Ground truth labels of shape (batch_size,).
        :return: Computed focal loss.
        """
        # Compute log-softmax
        log_probs = F.log_softmax(inputs, dim=1)
        probs = torch.exp(log_probs)
        
        # Gather the log probabilities corresponding to the target classes
        target_log_probs = log_probs.gather(1, targets.view(-1, 1)).squeeze(1)
        target_probs = probs.gather(1, targets.view(-1, 1)).squeeze(1)
        
        # Compute the focal weight (1 - p_t)^gamma
        focal_weight = (1 - target_probs) ** self.gamma
        
        # Compute the base cross entropy loss (negative log likelihood)
        loss = -target_log_probs * focal_weight
        
        # Apply class-wise alpha if given
        if self.alpha is not None:
            alpha_t = self.alpha.to(inputs.device)[targets]
            loss *= alpha_t
        
        # Apply reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


        

class LDAMLoss(nn.Module):
    def __init__(self, cls_num_list, max_m=0.5, s=30):
        super().__init__()
        self.m_list = torch.tensor(1.0 / np.sqrt(np.sqrt(cls_num_list)), dtype=torch.float32)
        self.m_list = self.m_list * (max_m / torch.max(self.m_list))
        self.s = s

    def forward(self, outputs, targets):
        index = torch.zeros_like(outputs, dtype=torch.bool)
        index.scatter_(1, targets.data.view(-1, 1), 1)

        # ğŸ”¹ self.m_listë¥¼ outputsê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        m_list = self.m_list.to(outputs.device)[targets].view(-1, 1)

        outputs_m = outputs - index * m_list  # Margin ì ìš©
        return F.cross_entropy(self.s * outputs_m, targets)

class Normalizer(): 
    def __init__(self, LpNorm=2, tau=1):
        self.LpNorm = LpNorm
        self.tau = tau
  
    def apply_on(self, model):  # tau-normalizationì„ classifier layerì— ì ìš©
        if hasattr(model, "fc"):  # fc ë ˆì´ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
            curLayer = model.fc.weight
        elif hasattr(model, "classifier"):  # classifier ë ˆì´ì–´ê°€ ìˆëŠ” ê²½ìš°
            curLayer = model.classifier.weight
        else:
            raise AttributeError("Model does not have 'fc' or 'classifier' attribute.")

        curparam = curLayer.data
        curparam_vec = curparam.reshape((curparam.shape[0], -1))
        neuronNorm_curparam = (torch.linalg.norm(curparam_vec, ord=self.LpNorm, dim=1)**self.tau).detach().unsqueeze(-1)
        scalingVect = torch.ones_like(curparam)    

        idx = neuronNorm_curparam == neuronNorm_curparam
        idx = idx.squeeze()
        tmp = 1 / (neuronNorm_curparam[idx].squeeze())
        for _ in range(len(scalingVect.shape)-1):
            tmp = tmp.unsqueeze(-1)

        scalingVect[idx] = torch.mul(scalingVect[idx], tmp)
        curparam[idx] = scalingVect[idx] * curparam[idx]



class MDCSLoss(nn.Module):
    def __init__(self, cls_num_list=None, max_m=0.5, s=30, tau=2):
        super().__init__()
        self.base_loss = F.cross_entropy

        prior = np.array(cls_num_list) #/ np.sum(cls_num_list)

        self.prior = torch.tensor(prior).float().cuda()
        self.C_number = len(cls_num_list)  # class number
        self.s = s
        self.tau = 2

        self.additional_diversity_factor = -0.2
        out_dim = 100
        self.register_buffer("center", torch.zeros(1, out_dim))
        self.register_buffer("center1", torch.zeros(1, out_dim))
        self.center_momentum = 0.9
        self.warmup = 20  
        self.reweight_epoch = 200
        if self.reweight_epoch != -1:
            idx = 1  # condition could be put in order to set idx
            betas = [0, 0.9999]
            effective_num = 1.0 - np.power(betas[idx], cls_num_list)
            per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)
            per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)
            self.per_cls_weights_enabled = torch.tensor(per_cls_weights, dtype=torch.float,
                                                        requires_grad=False)  # è¿™ä¸ªæ˜¯logitsæ—¶ç®—CE lossçš„weight
        self.per_cls_weights_enabled_diversity = torch.tensor(per_cls_weights, dtype=torch.float,
                                                              requires_grad=False).cuda()  # è¿™ä¸ªæ˜¯logitsæ—¶ç®—diversity lossçš„weight



    def _hook_before_epoch(self, epoch):
        if self.reweight_epoch != -1:
            self.epoch = epoch

            if epoch > self.reweight_epoch:
                self.per_cls_weights_base = self.per_cls_weights_enabled
                self.per_cls_weights_diversity = self.per_cls_weights_enabled_diversity
            else:
                self.per_cls_weights_base = None
                self.per_cls_weights_diversity = None

    def forward(self, output_logits, target, extra_info=None):
        if extra_info is None:
            return self.base_loss(output_logits, target)  # output_logits indicates the final prediction

        loss = 0
        temperature_mean = 1
        temperature = 1  
        # Obtain logits from each expert
        epoch = extra_info['epoch']
        num = int(target.shape[0] / 2)

        expert1_logits = extra_info['logits'][0] + torch.log(torch.pow(self.prior, -0.5) + 1e-9)      #head

        expert2_logits = extra_info['logits'][1] + torch.log(torch.pow(self.prior, 1) + 1e-9)         #medium

        expert3_logits = extra_info['logits'][2] + torch.log(torch.pow(self.prior, 2.5) + 1e-9)       #few



        teacher_expert1_logits = expert1_logits[:num, :]  # view1
        student_expert1_logits = expert1_logits[num:, :]  # view2

        teacher_expert2_logits = expert2_logits[:num, :]  # view1
        student_expert2_logits = expert2_logits[num:, :]  # view2

        teacher_expert3_logits = expert3_logits[:num, :]  # view1
        student_expert3_logits = expert3_logits[num:, :]  # view2




        teacher_expert1_softmax = F.softmax((teacher_expert1_logits) / temperature, dim=1).detach()
        student_expert1_softmax = F.log_softmax(student_expert1_logits / temperature, dim=1)

        teacher_expert2_softmax = F.softmax((teacher_expert2_logits) / temperature, dim=1).detach()
        student_expert2_softmax = F.log_softmax(student_expert2_logits / temperature, dim=1)

        teacher_expert3_softmax = F.softmax((teacher_expert3_logits) / temperature, dim=1).detach()
        student_expert3_softmax = F.log_softmax(student_expert3_logits / temperature, dim=1)


         

        teacher1_max, teacher1_index = torch.max(F.softmax((teacher_expert1_logits), dim=1).detach(), dim=1)
        student1_max, student1_index = torch.max(F.softmax((student_expert1_logits), dim=1).detach(), dim=1)

        teacher2_max, teacher2_index = torch.max(F.softmax((teacher_expert2_logits), dim=1).detach(), dim=1)
        student2_max, student2_index = torch.max(F.softmax((student_expert2_logits), dim=1).detach(), dim=1)

        teacher3_max, teacher3_index = torch.max(F.softmax((teacher_expert3_logits), dim=1).detach(), dim=1)
        student3_max, student3_index = torch.max(F.softmax((student_expert3_logits), dim=1).detach(), dim=1)


        # distillation
        partial_target = target[:num]
        kl_loss = 0
        if torch.sum((teacher1_index == partial_target)) > 0:
            kl_loss = kl_loss + F.kl_div(student_expert1_softmax[(teacher1_index == partial_target)],
                                         teacher_expert1_softmax[(teacher1_index == partial_target)],
                                         reduction='batchmean') * (temperature ** 2)

        if torch.sum((teacher2_index == partial_target)) > 0:
            kl_loss = kl_loss + F.kl_div(student_expert2_softmax[(teacher2_index == partial_target)],
                                         teacher_expert2_softmax[(teacher2_index == partial_target)],
                                         reduction='batchmean') * (temperature ** 2)

        if torch.sum((teacher3_index == partial_target)) > 0:
            kl_loss = kl_loss + F.kl_div(student_expert3_softmax[(teacher3_index == partial_target)],
                                         teacher_expert3_softmax[(teacher3_index == partial_target)],
                                         reduction='batchmean') * (temperature ** 2)

        loss = loss + 0.6 * kl_loss * min(extra_info['epoch'] / self.warmup, 1.0)



        # expert 1
        loss += self.base_loss(expert1_logits, target)

        # expert 2
        loss += self.base_loss(expert2_logits, target)

        # expert 3
        loss += self.base_loss(expert3_logits, target)


        return loss

    @torch.no_grad()
    def update_center(self, center, teacher_output):
        """
        Update center used for teacher output.
        """
        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)
        # dist.all_reduce(batch_center)
        batch_center = batch_center / (len(teacher_output))  # * dist.get_world_size())

        # ema update

        return center * self.center_momentum + batch_center * (1 - self.center_momentum)
    
    
class VSLoss(nn.Module):

    def __init__(self, cls_num_list, gamma=0.3, tau=1.0, weight=None):
        super(VSLoss, self).__init__()

        cls_probs = [cls_num / sum(cls_num_list) for cls_num in cls_num_list]
        temp = (1.0 / np.array(cls_num_list)) ** gamma
        temp = temp / np.min(temp)

        iota_list = tau * np.log(cls_probs)
        Delta_list = temp

        self.iota_list = torch.cuda.FloatTensor(iota_list)
        self.Delta_list = torch.cuda.FloatTensor(Delta_list)
        self.weight = weight

    def forward(self, x, target, use_multiplicative=True):
        output = x / self.Delta_list + self.iota_list if use_multiplicative else x + self.iota_list

        return F.cross_entropy(output, target, weight=self.weight)